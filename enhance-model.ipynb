{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-24T15:46:47.952137Z",
     "iopub.status.busy": "2025-12-24T15:46:47.951952Z",
     "iopub.status.idle": "2025-12-24T15:46:59.753711Z",
     "shell.execute_reply": "2025-12-24T15:46:59.753046Z",
     "shell.execute_reply.started": "2025-12-24T15:46:47.952121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "# ======================= Environment Setup & Global Configuration =======================\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "SCALE_FACTOR = 4     \n",
    "BATCH_SIZE = 16    \n",
    "LEARNING_RATE = 1e-4 \n",
    "NUM_EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:46:59.755098Z",
     "iopub.status.busy": "2025-12-24T15:46:59.754786Z",
     "iopub.status.idle": "2025-12-24T15:47:00.393320Z",
     "shell.execute_reply": "2025-12-24T15:47:00.392561Z",
     "shell.execute_reply.started": "2025-12-24T15:46:59.755079Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Loader Ready!\n",
      "Train Patches: 31180\n",
      "Val Patches: 3890\n"
     ]
    }
   ],
   "source": [
    "# ======================= Smart Super-Resolution Dataset & DataLoaders =======================\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train'):\n",
    "        self.hr_dir = os.path.join(root_dir, 'HR', split)\n",
    "        self.lr_dir = os.path.join(root_dir, 'LR', split)\n",
    "\n",
    "        self.image_filenames = [\n",
    "            x for x in os.listdir(self.hr_dir)\n",
    "            if x.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_filename = self.image_filenames[index]\n",
    "        hr_path = os.path.join(self.hr_dir, hr_filename)\n",
    "\n",
    "        name_no_ext = os.path.splitext(hr_filename)[0]\n",
    "\n",
    "        lr_path = None\n",
    "        for ext in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
    "            p = os.path.join(self.lr_dir, name_no_ext + ext)\n",
    "            if os.path.exists(p):\n",
    "                lr_path = p\n",
    "                break\n",
    "\n",
    "        if lr_path is None:\n",
    "            return self.__getitem__(np.random.randint(0, len(self.image_filenames)))\n",
    "\n",
    "        hr_img = Image.open(hr_path).convert('RGB')\n",
    "        lr_img = Image.open(lr_path).convert('RGB')\n",
    "\n",
    "        return TF.to_tensor(lr_img).to(device), TF.to_tensor(hr_img).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "\n",
    "DATASET_PATH = \"/kaggle/input/croped-data/Mini_Dataset_Smart_Kaggle\"\n",
    "\n",
    "train_ds = SRDataset(DATASET_PATH, split='train')\n",
    "val_ds = SRDataset(DATASET_PATH, split='val')\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Smart Loader Ready!\")\n",
    "print(f\"Train Patches: {len(train_ds)}\")\n",
    "print(f\"Val Patches: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:47:00.394331Z",
     "iopub.status.busy": "2025-12-24T15:47:00.394070Z",
     "iopub.status.idle": "2025-12-24T15:47:00.643813Z",
     "shell.execute_reply": "2025-12-24T15:47:00.643212Z",
     "shell.execute_reply.started": "2025-12-24T15:47:00.394305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ======================= SRResNet Generator Architecture (Residual Learning + PixelShuffle Upscaling) =======================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.prelu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, channels, scale_factor):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels * (scale_factor ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_res_blocks=16):\n",
    "        super(SRResNet, self).__init__()\n",
    "\n",
    "        self.conv_input = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=9, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(n_res_blocks)]\n",
    "        )\n",
    "\n",
    "        self.conv_mid = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            UpsampleBlock(64, 2),\n",
    "            UpsampleBlock(64, 2)\n",
    "        )\n",
    "\n",
    "        self.conv_output = nn.Conv2d(64, out_channels, kernel_size=9, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv_input(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out = self.conv_mid(out)\n",
    "        out = out + out1\n",
    "        out = self.upsample(out)\n",
    "        out = self.conv_output(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SRResNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "print(\"Model created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:47:00.645199Z",
     "iopub.status.busy": "2025-12-24T15:47:00.645009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|██████████| 1949/1949 [15:06<00:00,  2.15it/s, loss=0.0307, psnr=26]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 1 | Avg Loss: 0.04509 | Avg PSNR: 23.94 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]: 100%|██████████| 1949/1949 [11:04<00:00,  2.94it/s, loss=0.0329, psnr=25]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 2 | Avg Loss: 0.03201 | Avg PSNR: 25.92 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]: 100%|██████████| 1949/1949 [10:53<00:00,  2.98it/s, loss=0.0295, psnr=25.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 3 | Avg Loss: 0.02985 | Avg PSNR: 26.38 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/50]: 100%|██████████| 1949/1949 [11:01<00:00,  2.95it/s, loss=0.0244, psnr=28.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 4 | Avg Loss: 0.02876 | Avg PSNR: 26.60 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]: 100%|██████████| 1949/1949 [10:43<00:00,  3.03it/s, loss=0.0394, psnr=24.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 5 | Avg Loss: 0.02824 | Avg PSNR: 26.73 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/50]: 100%|██████████| 1949/1949 [10:52<00:00,  2.99it/s, loss=0.0365, psnr=23]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 6 | Avg Loss: 0.02783 | Avg PSNR: 26.83 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/50]: 100%|██████████| 1949/1949 [10:58<00:00,  2.96it/s, loss=0.0248, psnr=27.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 7 | Avg Loss: 0.02755 | Avg PSNR: 26.88 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/50]: 100%|██████████| 1949/1949 [10:58<00:00,  2.96it/s, loss=0.0182, psnr=29.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 8 | Avg Loss: 0.02724 | Avg PSNR: 26.97 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/50]: 100%|██████████| 1949/1949 [11:29<00:00,  2.83it/s, loss=0.0257, psnr=28.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 9 | Avg Loss: 0.02717 | Avg PSNR: 26.98 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/50]: 100%|██████████| 1949/1949 [11:39<00:00,  2.79it/s, loss=0.0256, psnr=27.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 10 | Avg Loss: 0.02689 | Avg PSNR: 27.05 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/50]: 100%|██████████| 1949/1949 [11:24<00:00,  2.85it/s, loss=0.0221, psnr=28.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 11 | Avg Loss: 0.02662 | Avg PSNR: 27.08 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/50]: 100%|██████████| 1949/1949 [11:18<00:00,  2.87it/s, loss=0.0345, psnr=25]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 12 | Avg Loss: 0.02659 | Avg PSNR: 27.09 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/50]: 100%|██████████| 1949/1949 [11:03<00:00,  2.94it/s, loss=0.0394, psnr=22.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 13 | Avg Loss: 0.02638 | Avg PSNR: 27.14 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/50]: 100%|██████████| 1949/1949 [11:10<00:00,  2.91it/s, loss=0.0283, psnr=26.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 14 | Avg Loss: 0.02632 | Avg PSNR: 27.15 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/50]: 100%|██████████| 1949/1949 [11:12<00:00,  2.90it/s, loss=0.0363, psnr=23.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 15 | Avg Loss: 0.02622 | Avg PSNR: 27.18 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/50]: 100%|██████████| 1949/1949 [11:07<00:00,  2.92it/s, loss=0.0332, psnr=25.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 16 | Avg Loss: 0.02615 | Avg PSNR: 27.21 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/50]: 100%|██████████| 1949/1949 [11:04<00:00,  2.93it/s, loss=0.0222, psnr=29]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 17 | Avg Loss: 0.02606 | Avg PSNR: 27.21 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/50]: 100%|██████████| 1949/1949 [10:53<00:00,  2.98it/s, loss=0.0297, psnr=24.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 18 | Avg Loss: 0.02589 | Avg PSNR: 27.26 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/50]: 100%|██████████| 1949/1949 [10:52<00:00,  2.99it/s, loss=0.0216, psnr=29]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 19 | Avg Loss: 0.02598 | Avg PSNR: 27.26 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/50]: 100%|██████████| 1949/1949 [10:59<00:00,  2.95it/s, loss=0.0213, psnr=30.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 20 | Avg Loss: 0.02584 | Avg PSNR: 27.24 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [21/50]: 100%|██████████| 1949/1949 [10:54<00:00,  2.98it/s, loss=0.0271, psnr=26.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Epoch 21 | Avg Loss: 0.02577 | Avg PSNR: 27.27 dB\n",
      " -> Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [22/50]:  84%|████████▍ | 1636/1949 [08:51<01:45,  2.96it/s, loss=0.0336, psnr=25]  "
     ]
    }
   ],
   "source": [
    "# ======================= SRResNet Training Loop (L1 Loss + PSNR Monitoring + Best Model Checkpoint) =======================\n",
    "\n",
    "def train_model():\n",
    "    print(\"Starting Training...\")\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_psnr = 0\n",
    "\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "\n",
    "        for lr_imgs, hr_imgs in loop:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sr_imgs = model(lr_imgs)\n",
    "            loss = criterion(sr_imgs, hr_imgs)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mse = nn.MSELoss()(sr_imgs, hr_imgs)\n",
    "                if mse == 0:\n",
    "                    psnr = 100\n",
    "                else:\n",
    "                    psnr = 10 * torch.log10(1 / mse)\n",
    "                train_psnr += psnr.item()\n",
    "\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "            loop.set_postfix(loss=loss.item(), psnr=psnr.item())\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        avg_psnr = train_psnr / len(train_loader)\n",
    "\n",
    "        print(f\" -> Epoch {epoch+1} | Avg Loss: {avg_loss:.5f} | Avg PSNR: {avg_psnr:.2f} dB\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"best_sr_model.pth\")\n",
    "            print(\" -> Model Saved!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================= Model Export =======================\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = SRResNet().to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/best_sr_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 48, 48).to(device)\n",
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "traced_model.save(\"final_model.pt\")\n",
    "\n",
    "print(\"Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T21:03:54.748923Z",
     "iopub.status.busy": "2025-12-27T21:03:54.748398Z",
     "iopub.status.idle": "2025-12-27T21:04:00.200944Z",
     "shell.execute_reply": "2025-12-27T21:04:00.200168Z",
     "shell.execute_reply.started": "2025-12-27T21:03:54.748900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      " Super Resolution image saved at: /kaggle/working/3_160_SR.png\n"
     ]
    }
   ],
   "source": [
    "# ======================= Inference =======================\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "MODEL_PATH = \"/kaggle/working/srresnet_scripted.pt\"\n",
    "INPUT_IMAGE = \"/kaggle/input/croped-data/Mini_Dataset_Smart_Kaggle/LR/test/3_160_aug.png\"\n",
    "OUTPUT_IMAGE = \"/kaggle/working/3_160_SR.png\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = torch.jit.load(MODEL_PATH, map_location=device)\n",
    "model.eval()\n",
    "\n",
    "img = Image.open(INPUT_IMAGE).convert(\"RGB\")\n",
    "lr_tensor = TF.to_tensor(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sr_tensor = model(lr_tensor)\n",
    "\n",
    "sr_tensor = sr_tensor.clamp(0, 1)\n",
    "sr_img = TF.to_pil_image(sr_tensor.squeeze(0))\n",
    "sr_img.save(OUTPUT_IMAGE)\n",
    "\n",
    "print(\"Super Resolution image saved at:\", OUTPUT_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T15:56:49.016285Z",
     "iopub.status.busy": "2025-12-29T15:56:49.015971Z",
     "iopub.status.idle": "2025-12-29T15:56:55.673160Z",
     "shell.execute_reply": "2025-12-29T15:56:55.672600Z",
     "shell.execute_reply.started": "2025-12-29T15:56:49.016260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =================================== FINE TUNING =======================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import vgg19\n",
    "from tqdm import tqdm\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# normalize for VGG19\n",
    "def normalize_vgg(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "    return (tensor - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T15:57:11.376120Z",
     "iopub.status.busy": "2025-12-29T15:57:11.375774Z",
     "iopub.status.idle": "2025-12-29T15:57:11.382313Z",
     "shell.execute_reply": "2025-12-29T15:57:11.381457Z",
     "shell.execute_reply.started": "2025-12-29T15:57:11.376100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    # =================================== INITIALIZATION ===================================\n",
    "    def __init__(self, root_dir, split='train'):\n",
    "        self.lr_dir = os.path.join(root_dir, 'LR', split)\n",
    "        self.hr_dir = os.path.join(root_dir, 'HR', split)\n",
    "\n",
    "        if not os.path.exists(self.lr_dir) or not os.path.exists(self.hr_dir):\n",
    "            raise FileNotFoundError(f\"Path not found: {root_dir}\")\n",
    "        self.images = sorted([f for f in os.listdir(self.lr_dir) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    # =================================== LENGTH ===================================\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    # =================================== GET ITEM ===================================\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path = os.path.join(self.lr_dir, self.images[idx])\n",
    "        hr_path = os.path.join(self.hr_dir, self.images[idx])\n",
    "        lr = Image.open(lr_path).convert('RGB')\n",
    "        hr = Image.open(hr_path).convert('RGB')\n",
    "        return TF.to_tensor(lr), TF.to_tensor(hr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T15:57:15.227791Z",
     "iopub.status.busy": "2025-12-29T15:57:15.227088Z",
     "iopub.status.idle": "2025-12-29T15:57:15.816935Z",
     "shell.execute_reply": "2025-12-29T15:57:15.816296Z",
     "shell.execute_reply.started": "2025-12-29T15:57:15.227762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders are ready! Train samples: 31180\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"/kaggle/input/croped-data/Mini_Dataset_Smart_Kaggle\"\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# =================================== DATASET ===================================\n",
    "train_ds = SRDataset(DATASET_PATH, split='train')  \n",
    "val_ds   = SRDataset(DATASET_PATH, split='val')\n",
    "\n",
    "# =================================== DATALOADERS ===================================\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Loaders are ready! Train samples: {len(train_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T15:57:19.962187Z",
     "iopub.status.busy": "2025-12-29T15:57:19.961909Z",
     "iopub.status.idle": "2025-12-29T15:57:19.973056Z",
     "shell.execute_reply": "2025-12-29T15:57:19.972390Z",
     "shell.execute_reply.started": "2025-12-29T15:57:19.962165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =================================== RESIDUAL BLOCK ===================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Basic residual block with two conv layers and skip connection.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)  # Skip connection\n",
    "\n",
    "# =================================== SRRESNET MODEL ===================================\n",
    "class SRResNet(nn.Module):\n",
    "    \"\"\"Super-Resolution Residual Network.\"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_feat=64, num_blocks=16):\n",
    "        super().__init__()\n",
    "        self.input_conv = nn.Sequential(nn.Conv2d(in_channels, num_feat, 9, 1, 4), nn.PReLU())\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(num_feat) for _ in range(num_blocks)])\n",
    "        self.mid_conv = nn.Sequential(nn.Conv2d(num_feat, num_feat, 3, 1, 1), nn.BatchNorm2d(num_feat))\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(num_feat, num_feat * 4, 3, 1, 1), nn.PixelShuffle(2), nn.PReLU(),\n",
    "            nn.Conv2d(num_feat, num_feat * 4, 3, 1, 1), nn.PixelShuffle(2), nn.PReLU()\n",
    "        )\n",
    "        self.output_conv = nn.Conv2d(num_feat, out_channels, 9, 1, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = self.input_conv(x)\n",
    "        res = self.res_blocks(x_in)\n",
    "        x = self.mid_conv(res) + x_in  # Residual connection\n",
    "        return self.output_conv(self.upsample(x))  # Final SR output\n",
    "\n",
    "# =================================== DISCRIMINATOR ===================================\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"PatchGAN-like discriminator for adversarial training.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def block(in_c, out_c, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, stride, 1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.LeakyReLU(0.2),\n",
    "            block(64, 64, 2), block(64, 128, 1), block(128, 128, 2),\n",
    "            block(128, 256, 1), block(256, 256, 2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Conv2d(256, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.net(x)).view(-1)  # Output probability per image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T15:57:26.800441Z",
     "iopub.status.busy": "2025-12-29T15:57:26.800174Z",
     "iopub.status.idle": "2025-12-29T15:57:30.913051Z",
     "shell.execute_reply": "2025-12-29T15:57:30.912140Z",
     "shell.execute_reply.started": "2025-12-29T15:57:26.800420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:02<00:00, 250MB/s] \n"
     ]
    }
   ],
   "source": [
    "# =================================== PERCEPTUAL LOSS ===================================\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual loss using pre-trained VGG19 features.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(weights='DEFAULT').features[:22].eval()\n",
    "        for p in vgg.parameters(): \n",
    "            p.requires_grad = False\n",
    "        self.vgg = vgg.to(device)\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        return F.mse_loss(self.vgg(normalize_vgg(sr)), self.vgg(normalize_vgg(hr)))\n",
    "\n",
    "# =================================== TOTAL VARIATION LOSS ===================================\n",
    "class TVLoss(nn.Module):\n",
    "    \"\"\"Total Variation Loss to enforce spatial smoothness.\"\"\"\n",
    "    def forward(self, x):\n",
    "        h_tv = torch.pow((x[:,:,1:,:] - x[:,:,:-1,:]), 2).sum() \n",
    "        w_tv = torch.pow((x[:,:,:,1:] - x[:,:,:,:-1]), 2).sum()\n",
    "        return (h_tv + w_tv) / (x.size(0) * x.size(1) * x.size(2) * x.size(3))\n",
    "\n",
    "# LOSS WEIGHTS\n",
    "λ_pix = 0.1 \n",
    "λ_per = 0.02  \n",
    "λ_adv = 0.005 \n",
    "λ_tv  = 1e-9    \n",
    "\n",
    "# LOSS FUNCTIONS\n",
    "pixel_loss_fn = nn.L1Loss()        \n",
    "vgg_loss_fn   = VGGPerceptualLoss()  \n",
    "adv_loss_fn   = nn.BCELoss()     \n",
    "tv_loss_fn    = TVLoss()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T15:57:34.985716Z",
     "iopub.status.busy": "2025-12-29T15:57:34.985364Z",
     "iopub.status.idle": "2025-12-29T15:57:35.147945Z",
     "shell.execute_reply": "2025-12-29T15:57:35.147160Z",
     "shell.execute_reply.started": "2025-12-29T15:57:34.985694Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained smooth model loaded!\n"
     ]
    }
   ],
   "source": [
    "model = SRResNet().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "SMOOTH_MODEL = \"/kaggle/input/best/pytorch/default/1/best_sr_model (1).pth\" \n",
    "if os.path.exists(SMOOTH_MODEL):\n",
    "    model.load_state_dict(torch.load(SMOOTH_MODEL, map_location=device), strict=False)\n",
    "    print(\"Pre-trained smooth model loaded!\")\n",
    "\n",
    "opt_G = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "opt_D = torch.optim.Adam(discriminator.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T15:57:54.844401Z",
     "iopub.status.busy": "2025-12-29T15:57:54.843852Z",
     "iopub.status.idle": "2025-12-29T22:48:49.348383Z",
     "shell.execute_reply": "2025-12-29T22:48:49.347462Z",
     "shell.execute_reply.started": "2025-12-29T15:57:54.844373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 1949/1949 [40:50<00:00,  1.26s/it, D_loss=1.1143, G_loss=0.2712]\n",
      "Epoch [2/10]: 100%|██████████| 1949/1949 [41:05<00:00,  1.26s/it, D_loss=0.0909, G_loss=0.5479]\n",
      "Epoch [3/10]: 100%|██████████| 1949/1949 [41:01<00:00,  1.26s/it, D_loss=0.1887, G_loss=0.2788]\n",
      "Epoch [4/10]: 100%|██████████| 1949/1949 [41:02<00:00,  1.26s/it, D_loss=0.1807, G_loss=0.3198]\n",
      "Epoch [5/10]: 100%|██████████| 1949/1949 [41:06<00:00,  1.27s/it, D_loss=0.1904, G_loss=0.3612]\n",
      "Epoch [6/10]: 100%|██████████| 1949/1949 [41:13<00:00,  1.27s/it, D_loss=1.1777, G_loss=0.3439]\n",
      "Epoch [7/10]: 100%|██████████| 1949/1949 [41:14<00:00,  1.27s/it, D_loss=0.2100, G_loss=0.4219]\n",
      "Epoch [8/10]: 100%|██████████| 1949/1949 [41:05<00:00,  1.27s/it, D_loss=0.8456, G_loss=0.2652]\n",
      "Epoch [9/10]: 100%|██████████| 1949/1949 [41:06<00:00,  1.27s/it, D_loss=0.2340, G_loss=0.3074]\n",
      "Epoch [10/10]: 100%|██████████| 1949/1949 [41:06<00:00,  1.27s/it, D_loss=0.2691, G_loss=0.2373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete with focus on skin texture!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =================================== TRAINING CONFIG ===================================\n",
    "EPOCHS = 10\n",
    "SAVE_DIR = \"/kaggle/working/models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# =================================== FINE-TUNING LOOP ===================================\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    discriminator.train()\n",
    "    loop = tqdm(train_loader)\n",
    "    \n",
    "    for lr_imgs, hr_imgs in loop:\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "\n",
    "        # ------------------- TRAIN DISCRIMINATOR -------------------\n",
    "        fake_sr = model(lr_imgs)\n",
    "        d_loss = adv_loss_fn(discriminator(hr_imgs), torch.ones(lr_imgs.size(0)).to(device)) + \\\n",
    "                 adv_loss_fn(discriminator(fake_sr.detach()), torch.zeros(lr_imgs.size(0)).to(device))\n",
    "        \n",
    "        opt_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        # ------------------- TRAIN GENERATOR -------------------\n",
    "        for _ in range(2):\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            fake_sr = model(lr_imgs)\n",
    "            \n",
    "            loss_pix = pixel_loss_fn(fake_sr, hr_imgs)\n",
    "            loss_per = vgg_loss_fn(fake_sr, hr_imgs) \n",
    "            loss_adv = adv_loss_fn(discriminator(fake_sr), torch.ones(lr_imgs.size(0)).to(device))\n",
    "            loss_tv  = tv_loss_fn(fake_sr)        \n",
    "\n",
    "            g_loss = (λ_pix * loss_pix) + (λ_per * loss_per) + (λ_adv * loss_adv) + (λ_tv * loss_tv)\n",
    "\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "        loop.set_postfix(G_loss=f\"{g_loss.item():.4f}\", D_loss=f\"{d_loss.item():.4f}\")\n",
    "\n",
    "    # Save model checkpoint per epoch\n",
    "    torch.save(model.state_dict(), f\"{SAVE_DIR}/srresnet_skin_focus_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"Fine-tuning complete with focus on skin texture!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T22:55:46.672948Z",
     "iopub.status.busy": "2025-12-29T22:55:46.672102Z",
     "iopub.status.idle": "2025-12-29T22:55:47.670731Z",
     "shell.execute_reply": "2025-12-29T22:55:47.669936Z",
     "shell.execute_reply.started": "2025-12-29T22:55:46.672910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "LAST_EPOCH_PATH = \"/kaggle/working/models/srresnet_skin_focus_epoch_10.pth\"\n",
    "\n",
    "model = SRResNet().to(device)\n",
    "model.load_state_dict(torch.load(LAST_EPOCH_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 48, 48).to(device)\n",
    "scripted_model = torch.jit.trace(model, dummy_input)\n",
    "scripted_model.save(\"/kaggle/working/model_fine.pt\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T23:48:48.646107Z",
     "iopub.status.busy": "2025-12-29T23:48:48.645347Z",
     "iopub.status.idle": "2025-12-29T23:52:39.284861Z",
     "shell.execute_reply": "2025-12-29T23:52:39.284252Z",
     "shell.execute_reply.started": "2025-12-29T23:48:48.646082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model with memory management on 27 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 5/27 [00:29<02:16,  6.22s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: UserWarning: cuDNN cannot be used for large non-batch-splittable convolutions if the V8 API is not enabled or before cuDNN version 9.3+. Consider upgrading cuDNN and/or enabling the V8 API for better efficiency. (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:430.)\n",
      "  return forward_call(*args, **kwargs)\n",
      " 22%|██▏       | 6/27 [00:33<01:58,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image too large for GPU, skipping or try resizing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 8/27 [00:51<02:09,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image too large for GPU, skipping or try resizing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [03:50<00:00,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Average PSNR: 28.51 dB\n",
      "Average SSIM: 0.8221\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_metric\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "\n",
    "# =================================== MEMORY CLEANUP ===================================\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# =================================== METRICS STORAGE ===================================\n",
    "psnr_values = []\n",
    "ssim_values = []\n",
    "\n",
    "print(f\"Evaluating model with memory management on {len(test_ds)} images...\")\n",
    "\n",
    "# =================================== EVALUATION LOOP ===================================\n",
    "with torch.no_grad():\n",
    "    for lr_img, hr_img in tqdm(test_loader):\n",
    "        lr_img = lr_img.to(device)\n",
    "        \n",
    "        try:\n",
    "            sr_img = model(lr_img).clamp(0, 1).cpu()\n",
    "            \n",
    "            sr_np = sr_img.squeeze(0).permute(1, 2, 0).numpy()\n",
    "            hr_np = hr_img.squeeze(0).permute(1, 2, 0).numpy()\n",
    "            \n",
    "            psnr_values.append(psnr_metric(hr_np, sr_np, data_range=1.0))\n",
    "            ssim_values.append(ssim_metric(hr_np, sr_np, data_range=1.0, channel_axis=2))\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"Image too large for GPU, skipping or try resizing.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        # Force cleanup after each image to free VRAM\n",
    "        del lr_img\n",
    "        if 'sr_img' in locals(): del sr_img\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# =================================== RESULTS ===================================\n",
    "if psnr_values:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Average PSNR: {np.mean(psnr_values):.2f} dB\")\n",
    "    print(f\"Average SSIM: {np.mean(ssim_values):.4f}\")\n",
    "    print(\"=\"*30)\n",
    "else:\n",
    "    print(\"No images were processed due to memory limits.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9032306,
     "sourceId": 14170231,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 543782,
     "modelInstanceId": 529792,
     "sourceId": 698430,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 545248,
     "modelInstanceId": 531419,
     "sourceId": 700383,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 547350,
     "modelInstanceId": 533666,
     "sourceId": 703182,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
